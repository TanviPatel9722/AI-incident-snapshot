{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "946cb274",
   "metadata": {},
   "source": [
    "# 03 Descriptive Statistics\n",
    "\n",
    "**Purpose**  \n",
    "Generate reproducible descriptive summaries for MIT, GMF, CSET, and report-corpus metadata while preserving original figure outputs.\n",
    "\n",
    "**Outputs preserved:** MIT/GMF/CSET distributions and report metadata plots in `../outputs/figures/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce89b41",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "PROJECT_ROOT = next((p for p in [Path.cwd(), *Path.cwd().parents] if (p / \"src\").exists()), Path.cwd())\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.notebook_utils import (\n",
    "    ensure_output_dir,\n",
    "    load_data,\n",
    "    normalize_incident_id,\n",
    "    pick_column_by_keywords,\n",
    "    plot_barh_top,\n",
    "    plot_percent_barh,\n",
    ")\n",
    "\n",
    "DATA_PATH = PROJECT_ROOT / \"data\"\n",
    "OUTPUT_PATH = ensure_output_dir(PROJECT_ROOT / \"outputs\" / \"figures\")\n",
    "TOP_N = 15\n",
    "DATE_CANDIDATES = [\"date_published\", \"date_submitted\", \"date_modified\", \"date_downloaded\"]\n",
    "REPORT_COLUMNS = [\"url\", \"source_domain\", \"language\", \"tags\", \"date_published\", \"date_submitted\", \"date_modified\", \"date_downloaded\"]\n",
    "\n",
    "loaded_tables = load_data(\n",
    "    DATA_PATH,\n",
    "    tables=[\"incidents\", \"reports\", \"mit\", \"gmf\", \"cset\"],\n",
    "    reports_usecols=REPORT_COLUMNS,\n",
    ")\n",
    "\n",
    "incidents_df = loaded_tables[\"incidents\"]\n",
    "reports_df = loaded_tables[\"reports\"]\n",
    "mit_df = normalize_incident_id(loaded_tables[\"mit\"])\n",
    "gmf_df = normalize_incident_id(loaded_tables[\"gmf\"])\n",
    "cset_df = normalize_incident_id(loaded_tables[\"cset\"])\n",
    "\n",
    "if incidents_df is None or reports_df is None:\n",
    "    raise FileNotFoundError(\"Required tables missing: incidents.csv or reports.csv.\")\n",
    "\n",
    "print(\"Incidents:\", incidents_df.shape)\n",
    "print(\"Reports (selected cols):\", reports_df.shape, \"cols:\", reports_df.columns.tolist())\n",
    "print(\"MIT:\", None if mit_df is None else mit_df.shape)\n",
    "print(\"GMF:\", None if gmf_df is None else gmf_df.shape)\n",
    "print(\"CSET:\", None if cset_df is None else cset_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d953091",
   "metadata": {},
   "source": [
    "## MIT Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f002fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mit_df is None:\n",
    "    print(\"MIT classifications not found.\")\n",
    "else:\n",
    "    for column_name, output_name in [\n",
    "        (\"risk_domain\", \"20_mit_risk_domain_top15.png\"),\n",
    "        (\"intent\", \"21_mit_intent_top15.png\"),\n",
    "        (\"entity\", \"22_mit_entity_top15.png\"),\n",
    "        (\"timing\", \"23_mit_timing_top15.png\"),\n",
    "    ]:\n",
    "        if column_name in mit_df.columns:\n",
    "            plot_barh_top(\n",
    "                mit_df[column_name],\n",
    "                title=f\"MIT: {column_name.replace('_', ' ').title()} (Top 15)\",\n",
    "                xlabel=\"Count\",\n",
    "                output_file=OUTPUT_PATH / output_name,\n",
    "                top_n=TOP_N,\n",
    "            )\n",
    "        else:\n",
    "            print(\"MIT missing column:\", column_name)\n",
    "\n",
    "    for column_name, output_name in [\n",
    "        (\"intent\", \"24_mit_intent_percent.png\"),\n",
    "        (\"entity\", \"25_mit_entity_percent.png\"),\n",
    "        (\"timing\", \"26_mit_timing_percent.png\"),\n",
    "    ]:\n",
    "        if column_name in mit_df.columns:\n",
    "            plot_percent_barh(\n",
    "                mit_df[column_name],\n",
    "                title=f\"MIT: {column_name.title()} Distribution (%)\",\n",
    "                output_file=OUTPUT_PATH / output_name,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89f0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mit_df is not None and \"risk_domain\" in mit_df.columns:\n",
    "    top_domains = mit_df[\"risk_domain\"].dropna().astype(str).value_counts().head(12).index\n",
    "\n",
    "    if \"intent\" in mit_df.columns:\n",
    "        mit_intent_df = mit_df.dropna(subset=[\"risk_domain\", \"intent\"]).copy()\n",
    "        mit_intent_df = mit_intent_df[mit_intent_df[\"risk_domain\"].astype(str).isin(top_domains)]\n",
    "        intent_crosstab_pct = (pd.crosstab(mit_intent_df[\"risk_domain\"], mit_intent_df[\"intent\"], normalize=\"index\") * 100).round(1)\n",
    "        print(\"MIT: Risk Domain x Intent (% within domain)\")\n",
    "        display(intent_crosstab_pct)\n",
    "\n",
    "    if \"entity\" in mit_df.columns:\n",
    "        mit_entity_df = mit_df.dropna(subset=[\"risk_domain\", \"entity\"]).copy()\n",
    "        mit_entity_df = mit_entity_df[mit_entity_df[\"risk_domain\"].astype(str).isin(top_domains)]\n",
    "        entity_crosstab_pct = (pd.crosstab(mit_entity_df[\"risk_domain\"], mit_entity_df[\"entity\"], normalize=\"index\") * 100).round(1)\n",
    "        print(\"MIT: Risk Domain x Entity (% within domain)\")\n",
    "        display(entity_crosstab_pct)\n",
    "else:\n",
    "    print(\"MIT risk_domain not found; skipping cross-tabs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cbc08c",
   "metadata": {},
   "source": [
    "## GMF and CSET Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c375df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gmf_df is None:\n",
    "    print(\"GMF classifications not found.\")\n",
    "else:\n",
    "    goal_col = pick_column_by_keywords(gmf_df, [\"goal\"], nice_to_have=[\"known\", \"ai\"])\n",
    "    tech_col = pick_column_by_keywords(gmf_df, [\"technology\"], nice_to_have=[\"known\", \"ai\"])\n",
    "    failure_col = pick_column_by_keywords(gmf_df, [\"failure\"], nice_to_have=[\"technical\", \"known\", \"ai\"])\n",
    "\n",
    "    print(\"Chosen GMF goal column:\", goal_col)\n",
    "    print(\"Chosen GMF tech column:\", tech_col)\n",
    "    print(\"Chosen GMF failure column:\", failure_col)\n",
    "\n",
    "    if goal_col:\n",
    "        plot_barh_top(gmf_df[goal_col], \"GMF: Known AI Goal (Top 15)\", \"Count\", OUTPUT_PATH / \"40_gmf_goal_top15.png\", top_n=TOP_N)\n",
    "    if tech_col:\n",
    "        plot_barh_top(gmf_df[tech_col], \"GMF: Known AI Technology (Top 15)\", \"Count\", OUTPUT_PATH / \"41_gmf_tech_top15.png\", top_n=TOP_N)\n",
    "    if failure_col:\n",
    "        plot_barh_top(gmf_df[failure_col], \"GMF: Known AI Technical Failure (Top 15)\", \"Count\", OUTPUT_PATH / \"42_gmf_failure_top15.png\", top_n=TOP_N)\n",
    "\n",
    "if cset_df is None:\n",
    "    print(\"CSET classifications not found.\")\n",
    "else:\n",
    "    cset_candidates = [\n",
    "        \"harm_distribution_basis\",\n",
    "        \"sector_of_deployment\",\n",
    "        \"harm_basis\",\n",
    "        \"sector\",\n",
    "        \"protected_class\",\n",
    "        \"basis\",\n",
    "    ]\n",
    "\n",
    "    found_any = False\n",
    "    for column_name in cset_candidates:\n",
    "        if column_name in cset_df.columns:\n",
    "            found_any = True\n",
    "            plot_barh_top(\n",
    "                cset_df[column_name],\n",
    "                title=f\"CSET: {column_name.replace('_', ' ').title()} (Top 15)\",\n",
    "                xlabel=\"Count\",\n",
    "                output_file=OUTPUT_PATH / f\"30_cset_{column_name}_top15.png\",\n",
    "                top_n=TOP_N,\n",
    "            )\n",
    "\n",
    "    if not found_any:\n",
    "        print(\"No expected CSET columns found in candidates list.\")\n",
    "        print(\"CSET columns:\", cset_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d132a1",
   "metadata": {},
   "source": [
    "## Report Corpus Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d6167",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"source_domain\" in reports_df.columns:\n",
    "    plot_barh_top(\n",
    "        reports_df[\"source_domain\"],\n",
    "        \"Report Corpus: Top Source Domains (Top 15)\",\n",
    "        \"Report count\",\n",
    "        OUTPUT_PATH / \"50_reports_top_source_domains.png\",\n",
    "        top_n=TOP_N,\n",
    "    )\n",
    "\n",
    "if \"language\" in reports_df.columns:\n",
    "    plot_barh_top(\n",
    "        reports_df[\"language\"],\n",
    "        \"Report Corpus: Languages (Top 15)\",\n",
    "        \"Report count\",\n",
    "        OUTPUT_PATH / \"51_reports_languages.png\",\n",
    "        top_n=TOP_N,\n",
    "    )\n",
    "\n",
    "if \"tags\" in reports_df.columns:\n",
    "    tags_series = reports_df[\"tags\"].dropna().astype(str)\n",
    "\n",
    "    # WHY: exploding unstructured long text can create large memory spikes.\n",
    "    if tags_series.str.len().median() > 200:\n",
    "        print(\"Tags appear text-heavy (median length > 200). Skipping explode-based tag analysis.\")\n",
    "    else:\n",
    "        exploded_tags = tags_series.str.split(\",\").explode().str.strip()\n",
    "        plot_barh_top(\n",
    "            exploded_tags,\n",
    "            \"Report Corpus: Tags (Top 15)\",\n",
    "            \"Count\",\n",
    "            OUTPUT_PATH / \"52_reports_tags.png\",\n",
    "            top_n=TOP_N,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c101f",
   "metadata": {},
   "source": [
    "## Interpretation and Limitations\n",
    "\n",
    "- Descriptive distributions are preserved exactly and remain non-causal.\n",
    "- Defensive checks prevent failures when optional taxonomy columns are absent.\n",
    "- Report metadata loading intentionally excludes heavy text to keep execution reliable on large snapshots."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}