{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ef928aa",
   "metadata": {},
   "source": [
    "# 07 Responsible Interpretation and Limitations\n",
    "\n",
    "**Purpose**  \n",
    "Document dataset scope, key constraints, and responsible interpretation guidance for all analytical outputs.\n",
    "\n",
    "This notebook intentionally avoids new computations beyond reporting core dataset diagnostics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9081144e",
   "metadata": {},
   "source": [
    "## Configuration and Dataset Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288bcf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from src.notebook_utils import load_data, normalize_incident_id\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"data\").exists() and (PROJECT_ROOT.parent / \"data\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "DATA_PATH = PROJECT_ROOT / \"data\"\n",
    "OUTPUT_PATH = PROJECT_ROOT / \"outputs\" / \"figures\"\n",
    "TOP_N = 15\n",
    "DATE_CANDIDATES = [\"date\", \"date_published\", \"date_submitted\"]\n",
    "\n",
    "loaded_tables = load_data(DATA_PATH, tables=[\"incidents\", \"reports\", \"submissions\", \"mit\", \"gmf\", \"cset\"])\n",
    "\n",
    "incidents_df = loaded_tables[\"incidents\"]\n",
    "reports_df = loaded_tables[\"reports\"]\n",
    "submissions_df = loaded_tables[\"submissions\"]\n",
    "mit_df = normalize_incident_id(loaded_tables[\"mit\"])\n",
    "gmf_df = normalize_incident_id(loaded_tables[\"gmf\"])\n",
    "cset_df = normalize_incident_id(loaded_tables[\"cset\"])\n",
    "\n",
    "if incidents_df is None or reports_df is None or submissions_df is None:\n",
    "    raise FileNotFoundError(\"Required tables missing: incidents.csv, reports.csv, or submissions.csv.\")\n",
    "\n",
    "incident_id_values = set(incidents_df[\"incident_id\"]) if \"incident_id\" in incidents_df.columns else set()\n",
    "\n",
    "overview_card = {\n",
    "    \"Incidents (rows)\": len(incidents_df),\n",
    "    \"Reports (rows)\": len(reports_df),\n",
    "    \"Unique report URLs\": reports_df[\"url\"].nunique() if \"url\" in reports_df.columns else None,\n",
    "    \"Unique source domains\": reports_df[\"source_domain\"].nunique() if \"source_domain\" in reports_df.columns else None,\n",
    "    \"Submissions rows (auxiliary)\": len(submissions_df),\n",
    "    \"MIT coverage (incident-level)\": (mit_df[\"incident_id\"].nunique() / len(incident_id_values)) if (mit_df is not None and \"incident_id\" in mit_df.columns and len(incident_id_values) > 0) else None,\n",
    "    \"GMF coverage (incident-level)\": (gmf_df[\"incident_id\"].nunique() / len(incident_id_values)) if (gmf_df is not None and \"incident_id\" in gmf_df.columns and len(incident_id_values) > 0) else None,\n",
    "    \"CSET coverage (incident-level)\": (cset_df[\"incident_id\"].nunique() / len(incident_id_values)) if (cset_df is not None and \"incident_id\" in cset_df.columns and len(incident_id_values) > 0) else None,\n",
    "}\n",
    "\n",
    "display(pd.DataFrame([overview_card]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4d13e",
   "metadata": {},
   "source": [
    "## What this dataset is (and is not)\n",
    "\n",
    "**What it is:**\n",
    "- A curated, documented collection of AI-related incidents and reporting artifacts.\n",
    "- Useful for identifying broad patterns in incident types, harms, and failure modes.\n",
    "- Useful for transparency and reproducible policy-facing analytics.\n",
    "\n",
    "**What it is not:**\n",
    "- A complete census of all AI harms.\n",
    "- A measurement of true prevalence of incidents across sectors.\n",
    "- A causal dataset (co-occurrence does not imply causality).\n",
    "\n",
    "## Key limitations and risks of misinterpretation\n",
    "\n",
    "1. **Selection and reporting bias**\n",
    "   - Incidents included depend on discoverability, media coverage, and monitoring.\n",
    "   - Certain regions or sectors may be under-represented.\n",
    "\n",
    "2. **Taxonomy coverage differences**\n",
    "   - MIT coverage is higher than GMF and CSET.\n",
    "   - Deep technical patterns may not generalize to all incidents.\n",
    "\n",
    "3. **Schema variation across snapshots**\n",
    "   - Column naming and completeness can differ between exports.\n",
    "   - Notebooks therefore use defensive column detection and explicit assumptions.\n",
    "\n",
    "4. **Temporal ambiguity**\n",
    "   - Incident dates and report publication dates measure different events.\n",
    "   - Reporting lag can only be approximated on subset joins.\n",
    "\n",
    "## Responsible usage guidance\n",
    "\n",
    "Appropriate:\n",
    "- Macro-level descriptive trends (domains, intent, failure types)\n",
    "- Transparency reporting and hypothesis generation\n",
    "- Reproducible policy-facing analytics\n",
    "\n",
    "Not appropriate:\n",
    "- Ranking organizations as most harmful\n",
    "- Making causal claims about architectures or actors\n",
    "- Estimating true global incidence rates\n",
    "\n",
    "## Reproducibility commitments\n",
    "\n",
    "- Notebooks run top-to-bottom with shared configuration and reusable helpers.\n",
    "- Outputs are saved to `outputs/figures/` and key tables are exportable.\n",
    "- Limitations are explicitly documented next to findings."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
